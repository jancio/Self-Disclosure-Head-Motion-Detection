# Offline head gesture detection for analysis of self-disclosure

This project was a contribution to the work on multimodal analysis and estimation of intimate self-disclosure. In particular, I trained head nod and head shake detectors. The detectors input head rotation angles (pitch and yaw) extracted from videos using OpenFace and output binary predictions of nod and shake head gestures on a frame-by-frame basis. This work led to a **conference paper**:

> Mohammad Soleymani, Kalin Stefanov, Sin-Hwa Kang, **Jan Ondras**, and Jonathan Gratch<br>
> [*Multimodal Analysis and Estimation of Intimate Self-Disclosure*](https://dl.acm.org/doi/abs/10.1145/3340555.3353737)<br>
> In 21st ACM International Conference on Multimodal Interaction (ACM ICMI), 2019<br>
> Oral presentation, *Best Paper Award*


### Citation

	@inproceedings{soleymani2019multimodal,
	  title={Multimodal Analysis and Estimation of Intimate Self-Disclosure},
	  author={Soleymani, Mohammad and Stefanov, Kalin and Kang, Sin-Hwa and Ondras, Jan and Gratch, Jonathan},
	  booktitle={2019 International Conference on Multimodal Interaction},
	  pages={59--68},
	  year={2019}
	}
